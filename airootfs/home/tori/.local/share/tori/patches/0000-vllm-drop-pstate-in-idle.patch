--- a/vllm/engine/metrics.py
+++ b/vllm/engine/metrics.py
@@ -7,6 +7,7 @@ from prometheus_client import (REGISTRY, Counter, Gauge, Histogram, Info,
                                disable_created_metrics)
 
 from vllm.logger import init_logger
+from nvidia_pstate import set_pstate_high, set_pstate_low
 
 if TYPE_CHECKING:
     from vllm.spec_decode.metrics import SpecDecodeWorkerMetrics
@@ -206,6 +207,9 @@ class StatLogger:
            Logs to prometheus and tracked stats every iteration.
            Logs to Stdout every self.local_interval seconds."""
 
+        # Set performance state.
+        set_pstate_low() if stats.num_running == 0 else set_pstate_high()
+
         # Log to prometheus.
         self._log_prometheus(stats)
 
--- a/vllm/entrypoints/openai/api_server.py
+++ b/vllm/entrypoints/openai/api_server.py
@@ -24,6 +24,7 @@ from vllm.entrypoints.openai.serving_chat import OpenAIServingChat
 from vllm.entrypoints.openai.serving_completion import OpenAIServingCompletion
 from vllm.logger import init_logger
 from vllm.usage.usage_lib import UsageContext
+from nvidia_pstate import set_pstate_high, set_pstate_low
 
 TIMEOUT_KEEP_ALIVE = 5  # seconds
 
@@ -87,6 +88,7 @@ async def show_version():
 @app.post("/v1/chat/completions")
 async def create_chat_completion(request: ChatCompletionRequest,
                                  raw_request: Request):
+    set_pstate_high()
     generator = await openai_serving_chat.create_chat_completion(
         request, raw_request)
     if isinstance(generator, ErrorResponse):
@@ -102,6 +104,7 @@ async def create_chat_completion(request: ChatCompletionRequest,
 
 @app.post("/v1/completions")
 async def create_completion(request: CompletionRequest, raw_request: Request):
+    set_pstate_high()
     generator = await openai_serving_completion.create_completion(
         request, raw_request)
     if isinstance(generator, ErrorResponse):
@@ -115,6 +118,7 @@ async def create_completion(request: CompletionRequest, raw_request: Request):
 
 
 if __name__ == "__main__":
+    set_pstate_low()
     args = parse_args()
 
     app.add_middleware(
--- a/vllm/worker/model_runner.py
+++ b/vllm/worker/model_runner.py
@@ -26,6 +26,7 @@ from vllm.sequence import (MultiModalData, SamplerOutput, SequenceData,
 from vllm.utils import (CudaMemoryProfiler, async_tensor_h2d, is_hip,
                         is_pin_memory_available, make_tensor_with_pad,
                         maybe_expand_dim)
+from nvidia_pstate import set_pstate_high, set_pstate_low
 
 logger = init_logger(__name__)
 
@@ -968,6 +969,7 @@ class ModelRunner:
         Since it is used for decoding-only, it assumes there's only 1 token
         per sequence in the batch.
         """
+        set_pstate_high()
         # NOTE(woosuk): This is a hack to ensure that the NCCL backend is never
         # deleted before the CUDA graphs.
         self.pynccl_backend = pynccl_utils.get_nccl_backend()
@@ -1055,6 +1057,7 @@ class ModelRunner:
         elapsed_time = end_time - start_time
         # This usually takes < 10 seconds.
         logger.info(f"Graph capturing finished in {elapsed_time:.0f} secs.")
+        set_pstate_low()
 
     def __del__(self) -> None:
         # Delete the CUDA graphs before deleting the pynccl communicator.
--- a/vllm/worker/worker.py
+++ b/vllm/worker/worker.py
@@ -21,6 +21,7 @@ from vllm.sequence import SamplerOutput, SequenceGroupMetadata
 from vllm.worker.cache_engine import CacheEngine
 from vllm.worker.model_runner import ModelRunner
 from vllm.worker.worker_base import WorkerBase
+from nvidia_pstate import set_pstate_high, set_pstate_low
 
 
 class Worker(WorkerBase):
@@ -129,6 +130,7 @@ class Worker(WorkerBase):
             You may limit the usage of GPU memory
             by adjusting the `gpu_memory_utilization` parameter.
         """
+        set_pstate_high()
         # Profile the memory usage of the model and get the maximum number of
         # cache blocks that can be allocated with the remaining free memory.
         torch.cuda.empty_cache()
@@ -144,6 +146,8 @@ class Worker(WorkerBase):
         # NOTE(woosuk): Here we assume that the other processes using the same
         # GPU did not change their memory usage during the profiling.
         peak_memory = self.init_gpu_memory - free_gpu_memory
+        if not peak_memory > 0:
+            set_pstate_low()
         assert peak_memory > 0, (
             "Error in memory profiling. This happens when the GPU memory was "
             "not properly cleaned up before initializing the vLLM instance.")
@@ -160,6 +164,7 @@ class Worker(WorkerBase):
             self.model_runner.remove_all_loras()
         gc.collect()
         torch.cuda.empty_cache()
+        set_pstate_low()
         return num_gpu_blocks, num_cpu_blocks
 
     def initialize_cache(self, num_gpu_blocks: int,
