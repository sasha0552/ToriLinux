                                            ██
     THIS PATCH IS OUTDATED AND           ██░░██
     HAS NOT BEEN UPDATED FOR A         ██░░░░░░██
     LONG TIME. IT WILL BE REMOVED.   ██░░░░░░░░░░██
                                      ██░░░░░░░░░░██
     PLEASE READ THE README.MD      ██░░░░░░░░░░░░░░██
     IN THIS DIRECTORY TO         ██░░░░░░██████░░░░░░██
     LEARN MORE.                  ██░░░░░░██████░░░░░░██
                                ██░░░░░░░░██████░░░░░░░░██
                                ██░░░░░░░░██████░░░░░░░░██
                              ██░░░░░░░░░░██████░░░░░░░░░░██
                            ██░░░░░░░░░░░░██████░░░░░░░░░░░░██
                            ██░░░░░░░░░░░░██████░░░░░░░░░░░░██
                          ██░░░░░░░░░░░░░░██████░░░░░░░░░░░░░░██
                          ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██
                        ██░░░░░░░░░░░░░░░░██████░░░░░░░░░░░░░░░░██
                        ██░░░░░░░░░░░░░░░░██████░░░░░░░░░░░░░░░░██
                      ██░░░░░░░░░░░░░░░░░░██████░░░░░░░░░░░░░░░░░░██
                      ██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░██
                        ██████████████████████████████████████████
--- a/vllm/engine/metrics.py
+++ b/vllm/engine/metrics.py
@@ -7,6 +7,7 @@ from typing import Dict, List, Optional, Protocol, Union
 
 import numpy as np
 import prometheus_client
+from nvidia_pstate import set_pstate_high, set_pstate_low
 
 from vllm.executor.ray_utils import ray
 from vllm.logger import init_logger
@@ -274,6 +275,9 @@ class LoggingStatLogger(StatLoggerBase):
         """Called by LLMEngine.
            Logs to Stdout every self.local_interval seconds."""
 
+        # Set performance state.
+        set_pstate_low() if stats.num_running_sys == 0 else set_pstate_high()
+
         # Save tracked stats for token counters.
         self.num_prompt_tokens.append(stats.num_prompt_tokens_iter)
         self.num_generation_tokens.append(stats.num_generation_tokens_iter)
--- a/vllm/entrypoints/openai/api_server.py
+++ b/vllm/entrypoints/openai/api_server.py
@@ -12,6 +12,7 @@ from fastapi import APIRouter, Request
 from fastapi.exceptions import RequestValidationError
 from fastapi.middleware.cors import CORSMiddleware
 from fastapi.responses import JSONResponse, Response, StreamingResponse
+from nvidia_pstate import set_pstate_high, set_pstate_low
 from prometheus_client import make_asgi_app
 from starlette.routing import Mount
 
@@ -121,6 +122,7 @@ async def show_version():
 @router.post("/v1/chat/completions")
 async def create_chat_completion(request: ChatCompletionRequest,
                                  raw_request: Request):
+    set_pstate_high()
     generator = await openai_serving_chat.create_chat_completion(
         request, raw_request)
     if isinstance(generator, ErrorResponse):
@@ -136,6 +138,7 @@ async def create_chat_completion(request: ChatCompletionRequest,
 
 @router.post("/v1/completions")
 async def create_completion(request: CompletionRequest, raw_request: Request):
+    set_pstate_high()
     generator = await openai_serving_completion.create_completion(
         request, raw_request)
     if isinstance(generator, ErrorResponse):
@@ -273,6 +276,7 @@ def run_server(args, llm_engine=None):
 
 
 if __name__ == "__main__":
+    set_pstate_low()
     # NOTE(simon):
     # This section should be in sync with vllm/scripts.py for CLI entrypoints.
     parser = FlexibleArgumentParser(
--- a/vllm/scripts.py
+++ b/vllm/scripts.py
@@ -5,6 +5,7 @@ import signal
 import sys
 from typing import Optional
 
+from nvidia_pstate import set_pstate_high, set_pstate_low
 from openai import OpenAI
 
 from vllm.entrypoints.openai.api_server import run_server
@@ -151,4 +152,5 @@ def main():
 
 
 if __name__ == "__main__":
+    set_pstate_low()
     main()
--- a/vllm/worker/model_runner.py
+++ b/vllm/worker/model_runner.py
@@ -10,6 +10,7 @@ import numpy as np
 import torch
 import torch.distributed
 import torch.nn as nn
+from nvidia_pstate import set_pstate_high, set_pstate_low
 
 try:
     from flashinfer import BatchDecodeWithPagedKVCacheWrapper
@@ -1003,6 +1004,7 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):
         Since it is used for decoding-only, it assumes there's only 1 token
         per sequence in the batch.
         """
+        set_pstate_high()
         assert not self.model_config.enforce_eager
         logger.info("Capturing the model for CUDA graphs. This may lead to "
                     "unexpected consequences if the model is not static. To "
@@ -1206,6 +1208,7 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):
         elapsed_time = end_time - start_time
         # This usually takes < 10 seconds.
         logger.info("Graph capturing finished in %.0f secs.", elapsed_time)
+        set_pstate_low()
 
     @property
     def vocab_size(self) -> int:
--- a/vllm/worker/worker.py
+++ b/vllm/worker/worker.py
@@ -5,6 +5,7 @@ from typing import List, Optional, Set, Tuple, Type
 
 import torch
 import torch.distributed
+from nvidia_pstate import set_pstate_high, set_pstate_low
 
 from vllm.config import (CacheConfig, DeviceConfig, LoadConfig, LoRAConfig,
                          ModelConfig, MultiModalConfig, ParallelConfig,
@@ -170,6 +171,7 @@ class Worker(LocalOrDistributedWorkerBase):
             You may limit the usage of GPU memory
             by adjusting the `gpu_memory_utilization` parameter.
         """
+        set_pstate_high()
         # Profile the memory usage of the model and get the maximum number of
         # cache blocks that can be allocated with the remaining free memory.
         torch.cuda.empty_cache()
@@ -185,6 +187,8 @@ class Worker(LocalOrDistributedWorkerBase):
         # NOTE(woosuk): Here we assume that the other processes using the same
         # GPU did not change their memory usage during the profiling.
         peak_memory = self.init_gpu_memory - free_gpu_memory
+        if not peak_memory > 0:
+            set_pstate_low()
         assert peak_memory > 0, (
             "Error in memory profiling. This happens when the GPU memory was "
             "not properly cleaned up before initializing the vLLM instance.")
@@ -201,6 +205,7 @@ class Worker(LocalOrDistributedWorkerBase):
             self.model_runner.remove_all_loras()
         gc.collect()
         torch.cuda.empty_cache()
+        set_pstate_low()
         return num_gpu_blocks, num_cpu_blocks
 
     def initialize_cache(self, num_gpu_blocks: int,
