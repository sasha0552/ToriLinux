--- a/vllm/engine/metrics.py
+++ b/vllm/engine/metrics.py
@@ -7,6 +7,7 @@ from typing import Dict, List, Optional, Protocol, Union
 
 import numpy as np
 import prometheus_client
+from nvidia_pstate import set_pstate_high, set_pstate_low
 
 from vllm.executor.ray_utils import ray
 from vllm.logger import init_logger
@@ -274,6 +275,9 @@ class LoggingStatLogger(StatLoggerBase):
         """Called by LLMEngine.
            Logs to Stdout every self.local_interval seconds."""
 
+        # Set performance state.
+        set_pstate_low() if stats.num_running_sys == 0 else set_pstate_high()
+
         # Save tracked stats for token counters.
         self.num_prompt_tokens.append(stats.num_prompt_tokens_iter)
         self.num_generation_tokens.append(stats.num_generation_tokens_iter)
--- a/vllm/entrypoints/openai/api_server.py
+++ b/vllm/entrypoints/openai/api_server.py
@@ -12,6 +12,7 @@ from fastapi import Request
 from fastapi.exceptions import RequestValidationError
 from fastapi.middleware.cors import CORSMiddleware
 from fastapi.responses import JSONResponse, Response, StreamingResponse
+from nvidia_pstate import set_pstate_high, set_pstate_low
 from prometheus_client import make_asgi_app
 from starlette.routing import Mount
 
@@ -129,6 +130,7 @@ async def show_version():
 @app.post("/v1/chat/completions")
 async def create_chat_completion(request: ChatCompletionRequest,
                                  raw_request: Request):
+    set_pstate_high()
     generator = await openai_serving_chat.create_chat_completion(
         request, raw_request)
     if isinstance(generator, ErrorResponse):
@@ -144,6 +146,7 @@ async def create_chat_completion(request: ChatCompletionRequest,
 
 @app.post("/v1/completions")
 async def create_completion(request: CompletionRequest, raw_request: Request):
+    set_pstate_high()
     generator = await openai_serving_completion.create_completion(
         request, raw_request)
     if isinstance(generator, ErrorResponse):
@@ -168,6 +171,7 @@ async def create_embedding(request: EmbeddingRequest, raw_request: Request):
 
 
 if __name__ == "__main__":
+    set_pstate_low()
     args = parse_args()
 
     app.add_middleware(
--- a/vllm/worker/model_runner.py
+++ b/vllm/worker/model_runner.py
@@ -10,6 +10,7 @@ import numpy as np
 import torch
 import torch.distributed
 import torch.nn as nn
+from nvidia_pstate import set_pstate_high, set_pstate_low
 
 try:
     from flashinfer import BatchDecodeWithPagedKVCacheWrapper
@@ -903,6 +904,7 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):
         Since it is used for decoding-only, it assumes there's only 1 token
         per sequence in the batch.
         """
+        set_pstate_high()
         assert not self.model_config.enforce_eager
         logger.info("Capturing the model for CUDA graphs. This may lead to "
                     "unexpected consequences if the model is not static. To "
@@ -1098,6 +1100,7 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):
         elapsed_time = end_time - start_time
         # This usually takes < 10 seconds.
         logger.info("Graph capturing finished in %.0f secs.", elapsed_time)
+        set_pstate_low()
 
     @property
     def vocab_size(self) -> int:
--- a/vllm/worker/worker.py
+++ b/vllm/worker/worker.py
@@ -5,6 +5,7 @@ from typing import List, Optional, Set, Tuple, Type
 
 import torch
 import torch.distributed
+from nvidia_pstate import set_pstate_high, set_pstate_low
 
 from vllm.config import (CacheConfig, DeviceConfig, LoadConfig, LoRAConfig,
                          ModelConfig, MultiModalConfig, ParallelConfig,
@@ -164,6 +165,7 @@ class Worker(LocalOrDistributedWorkerBase):
             You may limit the usage of GPU memory
             by adjusting the `gpu_memory_utilization` parameter.
         """
+        set_pstate_high()
         # Profile the memory usage of the model and get the maximum number of
         # cache blocks that can be allocated with the remaining free memory.
         torch.cuda.empty_cache()
@@ -179,6 +181,8 @@ class Worker(LocalOrDistributedWorkerBase):
         # NOTE(woosuk): Here we assume that the other processes using the same
         # GPU did not change their memory usage during the profiling.
         peak_memory = self.init_gpu_memory - free_gpu_memory
+        if not peak_memory > 0:
+            set_pstate_low()
         assert peak_memory > 0, (
             "Error in memory profiling. This happens when the GPU memory was "
             "not properly cleaned up before initializing the vLLM instance.")
@@ -195,6 +199,7 @@ class Worker(LocalOrDistributedWorkerBase):
             self.model_runner.remove_all_loras()
         gc.collect()
         torch.cuda.empty_cache()
+        set_pstate_low()
         return num_gpu_blocks, num_cpu_blocks
 
     def initialize_cache(self, num_gpu_blocks: int,
