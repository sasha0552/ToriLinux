--- a/koboldcpp.py
+++ b/koboldcpp.py
@@ -13,6 +13,7 @@ import os
 import argparse
 import json, sys, http.server, time, asyncio, socket, threading
 from concurrent.futures import ThreadPoolExecutor
+from nvidia_pstate import set_pstate_high, set_pstate_low
 
 sampler_order_max = 7
 stop_token_max = 16
@@ -399,6 +400,7 @@ def load_model(model_filename):
     return ret
 
 def generate(prompt, memory="", images=[], max_length=32, max_context_length=512, temperature=0.7, top_k=100, top_a=0.0, top_p=0.92, min_p=0.0, typical_p=1.0, tfs=1.0, rep_pen=1.0, rep_pen_range=128, presence_penalty=0.0, mirostat=0, mirostat_tau=5.0, mirostat_eta=0.1, sampler_order=[6,0,1,3,4,2,5], seed=-1, stop_sequence=[], use_default_badwordsids=False, stream_sse=False, grammar='', grammar_retain_state=False, genkey='', trimstop=False, quiet=False, dynatemp_range=0.0, dynatemp_exponent=1.0, smoothing_factor=0.0, logit_biases={}):
+    set_pstate_high()
     global maxctx, args, currentusergenkey, totalgens, pendingabortkey
     inputs = generation_inputs()
     inputs.prompt = prompt.encode("UTF-8")
@@ -491,6 +493,7 @@ def generate(prompt, memory="", images=[], max_length=32, max_context_length=512
     if pendingabortkey!="" and pendingabortkey==genkey:
         print(f"\nDeferred Abort for GenKey: {pendingabortkey}")
         pendingabortkey = ""
+        set_pstate_low()
         return ""
     else:
         ret = handle.generate(inputs)
@@ -502,6 +505,7 @@ def generate(prompt, memory="", images=[], max_length=32, max_context_length=512
                 sindex = outstr.find(trim_str)
                 if sindex != -1 and trim_str!="":
                     outstr = outstr[:sindex]
+        set_pstate_low()
         return outstr
 
 
@@ -2758,6 +2762,7 @@ def sanitize_string(input_string):
     return sanitized_string
 
 def main(launch_args,start_server=True):
+    set_pstate_low()
     global args, friendlymodelname, friendlysdmodelname, fullsdmodelpath, mmprojpath, password
     args = launch_args
     embedded_kailite = None
