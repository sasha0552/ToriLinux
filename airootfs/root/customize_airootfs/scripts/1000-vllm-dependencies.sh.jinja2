#!/bin/bash
set -eu

# vllm dependencies
pushd "vllm"
{% if CUDA %}
  # disable package caching
  export PIP_NO_CACHE_DIR=0

  # limit the number of parallel jobs to avoid OOM
  export MAX_JOBS=1

{% if CUDA %}
  # define supported architectures
  export TORCH_CUDA_ARCH_LIST="6.0 6.1 6.2 7.0 7.2 7.5 8.0 8.6 8.9 9.0 9.0+PTX"

  # cuda home directory
  export CUDA_HOME=/opt/cuda

  # use gcc 12
  export CC=gcc-12
  export CXX=g++-12
{% endif %}

{% if ROCm %}
  # define supported architectures
  export TORCH_CUDA_ARCH_LIST="gfx803 gfx900 gfx906 gfx908 gfx90a gfx1030 gfx1100 gfx1101 gfx1102"

  # rocm home directory
  export ROCM_HOME=/opt/rocm
{% endif %}

  # create venv
  python3 -m venv venv

  # activate venv
  source venv/bin/activate
{% if CUDA %}
    # install dependencies (cuda)
    pip3 install -r requirements.txt
{% endif %}

{% if ROCm %}
    # install dependencies (rocm)
    pip3 install -r requirements-rocm.txt
{% endif %}

    # install dependencies (build)
    pip3 install -r requirements-build.txt

    # build native extension
    python3 setup.py build_ext --inplace
  deactivate

  # remove venv
  rm -fr venv

  # create venv
  python3 -m venv venv

  # activate venv
  source venv/bin/activate
{% if CUDA %}
    # install dependencies (cuda)
    pip3 install -r requirements.txt
{% endif %}

{% if ROCm %}
    # install dependencies (rocm)
    pip3 install -r requirements-rocm.txt
{% endif %}

    # install dependencies for openai api server
    pip3 install accelerate
  deactivate
{% endif %}
popd
