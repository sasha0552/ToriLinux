#!/bin/sh
set -eu

# clone repository
git clone "{{ repositories.llamacpp }}.git"

# llama.cpp patches
pushd "llama.cpp"
  # use specific revision
  git checkout "{{ revisions.llamacpp }}"

  # create branch
  git checkout -b tori

  {% if platform == "cuda" %}
  # drop pstate in idle
  patch --no-backup-if-mismatch --strip 1 < "$TORI_PATCHES/0000-llamacpp-server-drop-pstate-in-idle.patch"
  {% endif %}

  {% if platform == "cuda" %}
  # commit changes
  git add .
  git commit -m "Apply patches"
  {% endif %}
popd

# llama.cpp dependencies
pushd "llama.cpp"
  # create venv
  python3 -m venv .venv

  # activate venv
  source .venv/bin/activate
    # use pytorch for cpu
    export PIP_EXTRA_INDEX_URL="https://download.pytorch.org/whl/cpu"

    # install dependencies
    pip3 install -r requirements.txt
  deactivate

  {% if platform == "cuda" %}
  ## populate ccache cache for faster builds

  # configure
  cmake                               \
    -Bbuild                           \
    -DCMAKE_CUDA_HOST_COMPILER=gcc-12 \
    -DCMAKE_CXX_COMPILER=g++-12       \
    -DCMAKE_C_COMPILER=gcc-12         \
    -DGGML_CUDA=ON

  # build
  cmake                               \
    --build build                     \
    --parallel $(nproc)

  # remove build outputs
  rm -r build/ common/build-info.cpp
  {% endif %}
popd
